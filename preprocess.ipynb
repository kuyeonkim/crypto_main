{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOLCORR_PATH = Path('data/volcorrultimate.csv')\n",
        "TOTAL_DF_PATH = Path('data/total_df_tier_mean_spread.csv')\n",
        "OTHER_DATA_PATH = Path('data/other_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_volcorr(path: Path = VOLCORR_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Load the volcorrultimate dataset.\"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def preprocess_volcorr(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Preprocess volcorrultimate data.\n",
        "\n",
        "    - Drop missing rows\n",
        "    - Replace zero p-values with the smallest positive value\n",
        "    - Apply -log10 transformation to p-values\n",
        "    - Standard scale beta columns and robust scale p-value columns\n",
        "    \"\"\"\n",
        "    df = df.dropna().copy()\n",
        "\n",
        "    pval_cols = ['abs_ret_pval', 'pos_pval', 'interact_pval']\n",
        "    for col in pval_cols:\n",
        "        m = df.loc[df[col] > 0, col].min()\n",
        "        df.loc[df[col] == 0, col] = m\n",
        "        df[col + '_transformed'] = -np.log10(df[col])\n",
        "\n",
        "    beta = df[['beta1', 'beta2', 'beta3']].values\n",
        "    pvals = df[[c + '_transformed' for c in pval_cols]].values\n",
        "\n",
        "    beta_scaled = StandardScaler().fit_transform(beta)\n",
        "    pval_scaled = RobustScaler().fit_transform(pvals)\n",
        "\n",
        "    X = np.hstack([beta_scaled, pval_scaled])\n",
        "    return pd.DataFrame(X, columns=['beta1', 'beta2', 'beta3'] + [c + '_t' for c in pval_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_total_df(path: Path = TOTAL_DF_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Load the market cap correlation dataset.\"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def preprocess_total_df(df: pd.DataFrame, scalemethod: str = 'standard') -> pd.DataFrame:\n",
        "    \"\"\"Preprocess the total_df dataset.\n",
        "\n",
        "    This mirrors the transformations from the notebook and keeps the\n",
        "    column structure intact so that later steps can consume the output.\n",
        "    \"\"\"\n",
        "    cap_list = ['Large', 'Medium', 'Small', 'Micro']\n",
        "\n",
        "    pairs = []\n",
        "    for i in range(len(cap_list)):\n",
        "        for j in range(i + 1, len(cap_list)):\n",
        "            pairs.append((cap_list[i], cap_list[j]))\n",
        "\n",
        "    mean_cols = [f\"{cap}_mean\" for cap in cap_list]\n",
        "    sd_cols = [f\"{cap}_sd\" for cap in cap_list]\n",
        "    diff_cols = [f\"{c1}_{c2}_diff\" for c1, c2 in pairs]\n",
        "    corr_cols = [f\"{c1}_{c2}_corr\" for c1, c2 in pairs]\n",
        "\n",
        "    scaler = StandardScaler() if scalemethod == 'standard' else RobustScaler()\n",
        "    signed_log = lambda x: np.sign(x) * np.log1p(abs(x))\n",
        "\n",
        "    cap_mean_df = df[['exchange'] + mean_cols].copy().set_index('exchange')\n",
        "    cap_sd_df = df[['exchange'] + sd_cols].copy().set_index('exchange')\n",
        "    cap_diff_df = df[['exchange'] + diff_cols].copy().set_index('exchange')\n",
        "    cap_corr_df = df[['exchange'] + corr_cols].copy().set_index('exchange')\n",
        "\n",
        "    cap_mean_df = pd.DataFrame(\n",
        "        scaler.fit_transform(signed_log(cap_mean_df)),\n",
        "        columns=cap_mean_df.columns,\n",
        "        index=cap_mean_df.index,\n",
        "    )\n",
        "    cap_sd_df = pd.DataFrame(\n",
        "        scaler.fit_transform(signed_log(cap_sd_df)),\n",
        "        columns=cap_sd_df.columns,\n",
        "        index=cap_sd_df.index,\n",
        "    )\n",
        "    cap_diff_df = pd.DataFrame(\n",
        "        scaler.fit_transform(signed_log(cap_diff_df)),\n",
        "        columns=cap_diff_df.columns,\n",
        "        index=cap_diff_df.index,\n",
        "    )\n",
        "\n",
        "    cap_mean_df = cap_mean_df.dropna(axis=0)\n",
        "    cap_sd_df = cap_sd_df.dropna(axis=0)\n",
        "    cap_diff_df = cap_diff_df.dropna(axis=0)\n",
        "    cap_corr_df = cap_corr_df.dropna(axis=0)\n",
        "\n",
        "    processed = pd.concat([cap_mean_df, cap_sd_df, cap_corr_df], axis=1)\n",
        "    processed = processed.dropna()\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_other(path: Path = OTHER_DATA_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Load the third dataset (placeholder).\"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def preprocess_other(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Placeholder preprocessing for the third dataset.\"\"\"\n",
        "    return df.dropna()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rsch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
