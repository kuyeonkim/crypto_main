{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Data paths"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "VOLCORR_PATH = Path('data/volcorrultimate.csv')\n",
    "TOTAL_DF_PATH = Path('data/total_df_tier_mean_spread.csv')\n",
    "OTHER_DATA_PATH = Path('data/other_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### volcorrultimate.csv preprocessing"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_volcorr(path: Path = VOLCORR_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the volcorrultimate dataset.\"\"\"\n",
    "    return pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_volcorr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess volcorrultimate data.\n",
    "\n",
    "    - Drop missing rows\n",
    "    - Replace zero p-values with the smallest positive value\n",
    "    - Apply -log10 transformation to p-values\n",
    "    - Standard scale beta columns and robust scale p-value columns\n",
    "    \"\"\"\n",
    "    df = df.dropna().copy()\n",
    "\n",
    "    pval_cols = ['abs_ret_pval', 'pos_pval', 'interact_pval']\n",
    "    for col in pval_cols:\n",
    "        m = df.loc[df[col] > 0, col].min()\n",
    "        df.loc[df[col] == 0, col] = m\n",
    "        df[col + '_transformed'] = -np.log10(df[col])\n",
    "\n",
    "    beta = df[['beta1', 'beta2', 'beta3']].values\n",
    "    pvals = df[[c + '_transformed' for c in pval_cols]].values\n",
    "\n",
    "    beta_scaled = StandardScaler().fit_transform(beta)\n",
    "    pval_scaled = RobustScaler().fit_transform(pvals)\n",
    "\n",
    "    X = np.hstack([beta_scaled, pval_scaled])\n",
    "    return pd.DataFrame(X, columns=['beta1', 'beta2', 'beta3'] + [c + '_t' for c in pval_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### total_df_tier_mean_spread.csv preprocessing"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_total_df(path: Path = TOTAL_DF_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the market cap correlation dataset.\"\"\"\n",
    "    return pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_total_df(df: pd.DataFrame, scalemethod: str = 'standard') -> pd.DataFrame:\n",
    "    \"\"\"Preprocess the total_df dataset.\n",
    "\n",
    "    This mirrors the transformations from the notebook and keeps the\n",
    "    column structure intact so that later steps can consume the output.\n",
    "    \"\"\"\n",
    "    cap_list = ['Large', 'Medium', 'Small', 'Micro']\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(len(cap_list)):\n",
    "        for j in range(i + 1, len(cap_list)):\n",
    "            pairs.append((cap_list[i], cap_list[j]))\n",
    "\n",
    "    mean_cols = [f\"{cap}_mean\" for cap in cap_list]\n",
    "    sd_cols = [f\"{cap}_sd\" for cap in cap_list]\n",
    "    diff_cols = [f\"{c1}_{c2}_diff\" for c1, c2 in pairs]\n",
    "    corr_cols = [f\"{c1}_{c2}_corr\" for c1, c2 in pairs]\n",
    "\n",
    "    scaler = StandardScaler() if scalemethod == 'standard' else RobustScaler()\n",
    "    signed_log = lambda x: np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "    cap_mean_df = df[['exchange'] + mean_cols].copy().set_index('exchange')\n",
    "    cap_sd_df = df[['exchange'] + sd_cols].copy().set_index('exchange')\n",
    "    cap_diff_df = df[['exchange'] + diff_cols].copy().set_index('exchange')\n",
    "    cap_corr_df = df[['exchange'] + corr_cols].copy().set_index('exchange')\n",
    "\n",
    "    cap_mean_df = pd.DataFrame(\n",
    "        scaler.fit_transform(signed_log(cap_mean_df)),\n",
    "        columns=cap_mean_df.columns,\n    "        index=cap_mean_df.index,\n    "    )\n",
    "    cap_sd_df = pd.DataFrame(\n",
    "        scaler.fit_transform(signed_log(cap_sd_df)),\n",
    "        columns=cap_sd_df.columns,\n",
    "        index=cap_sd_df.index,\n    "    )\n",
    "    cap_diff_df = pd.DataFrame(\n",
    "        scaler.fit_transform(signed_log(cap_diff_df)),\n",
    "        columns=cap_diff_df.columns,\n    "        index=cap_diff_df.index,\n    "    )\n",
    "\n",
    "    cap_mean_df = cap_mean_df.dropna(axis=0)\n",
    "    cap_sd_df = cap_sd_df.dropna(axis=0)\n",
    "    cap_diff_df = cap_diff_df.dropna(axis=0)\n",
    "    cap_corr_df = cap_corr_df.dropna(axis=0)\n",
    "\n",
    "    processed = pd.concat([cap_mean_df, cap_sd_df, cap_corr_df], axis=1)\n",
    "    processed = processed.dropna()\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Placeholder for the third dataset"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_other(path: Path = OTHER_DATA_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the third dataset (placeholder).\"\"\"\n",
    "    return pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_other(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Placeholder preprocessing for the third dataset.\"\"\"\n",
    "    return df.dropna()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
